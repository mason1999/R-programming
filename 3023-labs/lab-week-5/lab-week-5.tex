\documentclass[12pt, a4paper]{article}
\input{./everything/everything.tex}

% for the mathscr
\usepackage{mathrsfs}
% easier for bold symbols
\newcommand{\bs}[1]{\boldsymbol{#1}}
% partial derivatives
\newcommand{\partiald}[1]{\frac{\delta}{\delta#1}}
% for estimators
\newcommand{\wh}[1]{\widehat{#1}}
% for examples
\newcommand{\gb}[1]{\greybox{#1}}


\titleformat{\section}{\normalfont\Large\bfseries}{}{0pt}{}
% for sets and curly letters
\newcommand{\cur}[1]{\mathcal{#1}}
\newcommand{\scr}[1]{\mathscr{#1}}


\usepackage{Sweave}
\begin{document}
\input{lab-week-5-concordance}
% refer to https://yihui.org/knitr/options/#code-decoration for more options
\begin{Schunk}
\begin{Sinput}
> knitr::opts_chunk$set(
+   comment = '', fig.width = 6, fig.height = 4, fig.align = "center",
+   tide = TRUE, size = "small"
+ )
\end{Sinput}
\end{Schunk}

% (1) Remember that we want the default page style
% (2) but for this page we want an empty page style for the title 
% (3) we input our title 
% (4) we call a new page and reset the page counter to 1
\pagestyle{default}
\thispagestyle{empty}
\input{./title_page.tex}
\newpage
\setcounter{page}{1}
We shall compare three different estimators of a binomial success probability. If $Y \thicksim B(2, \theta)$ then we have: $P(Y = 0) = (1 - \theta^2)$, $P(Y = 1) = 2\theta(1 - \theta)$, $P(Y = 2) = \theta^2$. Moreover, if we have an iid sample $Y_1, Y_2,..., Y_n$ then if we define:

\begin{itemize}
  \item $N_0 = \sum_{i = 1}^n1_{\{Y_i = 0\}}$ as the number of 0's $\implies N_0 \thicksim B(n, (1 - \theta)^2)$
  \item $N_1 = \sum_{i = 1}^n1_{\{Y_i = 1\}}$ as the number of 1's  $\implies N_1\thicksim B(n, 2\theta(1 - \theta))$
  \item $N_2 = \sum_{i = 1}^n1_{\{Y_i = 2\}}$ as the number of 2's $\implies N_2 \thicksim B(n, \theta^2)$
\end{itemize}

The usual estimator of $\theta$ based on an iid sample $Y_1, Y_2, ... , Y_n$ is a function of $\overbar{1}{Y}= \frac{1}{n}\sum_{i = 1}^nY_i$

\begin{enumerate}[label={\bfseries\arabic*.}]
\item Determine an unbiased estimator of $\theta$ which is a \textit{linear} function of $\overbar{1}{Y}$. Call it $\hat{\theta_1}$

{\setlength{\leftskip}{3ex}
\tbf{Solution}

To find an unbiased estimator of $\theta$ we first note that:
\begin{align*}
\E[\overbar{0.5}{Y}] & = \frac{1}{n}\sum_{i = 1}^n\E[Y_1]\\
& = \E[Y_1] \\
& = 2\theta
\end{align*}
Hence we should define an unbiased estimator $\hat{\theta_1}$ by: 
$$\hat{\theta_1} = \frac{1}{2}\overbar{0.2}{Y}$$
}
\item Determine an unbiased estimator of $\theta$ which is a \textit{nonlinear} function of $N_0$ (hint: use method of moments, i.e. set equal to expectation and solve for $\theta$). Call it $\hat{\theta_0}$

{\setlength{\leftskip}{3ex}
\tbf{Solution}

To find an unbiased estimator of $\theta$ we recall the method of moments. We have that $1_{\{Y_i = 0\}} \thicksim bernoulli((1 - \theta)^2)$. Hence we have that $\E\brac{1_{\{Y_i = 0\}}} = (1 - \theta)^2$. With the random sample $1_{\{Y_1 = 0\}}, 1_{\{Y_2 = 0\}}, ..., 1_{\{Y_n = 0\}}$. We have that (by the method of moments) $ (1 - \theta)^2 = \frac{1}{n}\sum_{i = 1}^n 1_{\{Y_i = 0\}} \implies (1 - \theta)^2 = \frac{1}{n}N_0 \implies \hat{\theta_0} = 1 - \sqrt{\frac{N_0}{n}}$

}

\item Determine an unbiased estimator of $\theta$ which is a \textit{nonlinear} function of $N_2$ (hint: use method of moments, i.e. set equal to expectation and solve for $\theta$). Call it $\hat{\theta_2}$

{\setlength{\leftskip}{3ex}
\tbf{Solution}

To find an unbiased estimator of $\theta$ we recall the method of moments. We have that $1_{\{Y_i = 2\}} \thicksim bernoulli(\theta^2)$. Hence we have that $\E\brac{1_{\{Y_i = 2\}}} =\theta^2$. With the random sample $1_{\{Y_1 = 2\}}, 1_{\{Y_2 = 2\}},$ 

$..., 1_{\{Y_n = 2\}}$. We have that (by the method of moments) $ \theta^2 = \frac{1}{n}\sum_{i = 1}^n 1_{\{Y_i = 2\}} \implies \theta^2 = \frac{1}{n}N_2 \implies \hat{\theta_2} = \sqrt{\frac{N_2}{n}}$

}

\item We shall simulate a sample if $n = 100$ iid such $Y_i$â€™s and compute the values of these three estimators and then compare their mean squared errors, for a fine grid of $\theta$ values.

{\setlength{\leftskip}{3ex}
\tbf{Solution}

<<<<<<< HEAD
We want to now compare the variance of $\hat{\theta_0}, \hat{\theta_1}$ and $\hat{\theta_2}$ with the CRLB of $\frac{\theta\rbrac{1 - \theta}}{2n}$. To do this, we illustrate the idea with $\hat{\theta_0}$ first. 

\begin{Schunk}
\begin{Sinput}
> # create a partiton of theta values
> theta_vals = (1:39)/40
> # create a vector to store the theta_0 values we obtain
> var_theta_0_for_each_theta = rep(0, length(theta_vals))
> # set the sample size n and the number of simulation iterations N
> n = 100
> N = 1000
> # Define the MSE0 to be zero 
> MSE0 = 0
> # define a vector of length 1000 so store our simulation values
> simulate_N = rep(0, N)
> # for each value of theta_vals, we want to estimate the variance of theta_0. 
> for (i in 1:length(theta_vals)) {
+   # We draw n times from Y ~ B(2, theta_vals[1]) and calculate theta_1_hat. 
+   # We simulate this experiment N times. 
+   
+   # temp_vect = rbinom(n * N, 2, theta_vals[i])
+   # mat_sim = matrix(temp_vect, ncols = n, nrows = N)
+   # mat_sim = mat_sim == 0
+   # theta_0_estimates = apply(mat_sim, MARGIN = 1, FUN = sum)
+   # theta_0_estimates = sqrt(theta_0_estimates/n)
+   # var_theta_0_for_each_theta[i] = sum((theta_0_estimates - theta_vals[i])^2)/N
+   
+   for (ii in 1:N) {
+     temp_vect = rbinom(n, 2, theta_vals[i])
+     temp_vect = temp_vect == 0
+     simulate_N[ii] = 1 - sqrt(sum(temp_vect)/n)
+   }
+   
+   var_theta_0_for_each_theta[i] = sum((simulate_N - theta_vals[i])^2)/N
+   
+   
+ }
> var_theta_0_for_each_theta
\end{Sinput}
\begin{Soutput}
 [1] 0.0001298333 0.0002541709 0.0003738888 0.0004728320 0.0005869841
 [6] 0.0007200118 0.0008558126 0.0008858230 0.0010000302 0.0010605262
[11] 0.0012217792 0.0012858581 0.0013935799 0.0015341275 0.0015389720
[16] 0.0017292130 0.0016570296 0.0017520842 0.0017844477 0.0019746090
[21] 0.0018957917 0.0021606012 0.0022098312 0.0022712450 0.0021134446
[26] 0.0023292737 0.0024023104 0.0023773007 0.0024837647 0.0026832055
[31] 0.0027801322 0.0031332682 0.0034612553 0.0037073586 0.0042293449
[36] 0.0047323146 0.0040348470 0.0026634800 0.0009129289
\end{Soutput}
\begin{Sinput}
> plot(theta_vals, var_theta_0_for_each_theta)
\end{Sinput}
\end{Schunk}
=======
We want to now compare the variance of $\hat{\theta_0}, \hat{\theta_1}$ and $\hat{\theta_2}$ with the CRLB of $\frac{\theta\rbrac{1 - \theta}}{2n}$. 

\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# sample size}
\hlstd{n} \hlkwb{=} \hlnum{100}
\hlcom{# Number of simulation iterations}
\hlstd{N} \hlkwb{=} \hlnum{1000}
\hlcom{# theta values and length of this vector of values}
\hlstd{thetaVals} \hlkwb{=} \hlstd{(}\hlnum{1}\hlopt{:}\hlnum{39}\hlstd{)}\hlopt{/}\hlnum{40}
\hlstd{len} \hlkwb{=} \hlkwd{length}\hlstd{(thetaVals)}
\hlcom{# Declare the mean squared error vectors to be plotted against the theta values}
\hlstd{mse0} \hlkwb{=} \hlkwd{vector}\hlstd{(}\hlkwc{mode} \hlstd{=} \hlstr{"numeric"}\hlstd{,} \hlkwc{length} \hlstd{= len)}
\hlstd{mse1} \hlkwb{=} \hlkwd{vector}\hlstd{(}\hlkwc{mode} \hlstd{=} \hlstr{"numeric"}\hlstd{,} \hlkwc{length} \hlstd{= len)}
\hlstd{mse2} \hlkwb{=} \hlkwd{vector}\hlstd{(}\hlkwc{mode} \hlstd{=} \hlstr{"numeric"}\hlstd{,} \hlkwc{length} \hlstd{= len)}

\hlcom{# Declare temporary variables to be used in the loop}
\hlstd{temp0} \hlkwb{=} \hlkwd{vector}\hlstd{(}\hlkwc{mode} \hlstd{=} \hlstr{"numeric"}\hlstd{,} \hlkwc{length} \hlstd{= N)}
\hlstd{temp1} \hlkwb{=} \hlkwd{vector}\hlstd{(}\hlkwc{mode} \hlstd{=} \hlstr{"numeric"}\hlstd{,} \hlkwc{length} \hlstd{= N)}
\hlstd{temp2} \hlkwb{=} \hlkwd{vector}\hlstd{(}\hlkwc{mode} \hlstd{=} \hlstr{"numeric"}\hlstd{,} \hlkwc{length} \hlstd{= N)}

\hlkwa{for} \hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlstd{len) \{}
  \hlcom{# We set the current value of theta. }
  \hlstd{currThetaValue} \hlkwb{=} \hlstd{thetaVals[i]}

  \hlcom{# For the current theta value, we calculate the mean squared error for all }
  \hlcom{# three estimators}
  \hlkwa{for} \hlstd{(j} \hlkwa{in} \hlnum{1}\hlopt{:}\hlstd{N) \{}

    \hlcom{# step 1: Draw a sample of n binomal observations}
    \hlstd{obs} \hlkwb{=} \hlkwd{rbinom}\hlstd{(}\hlkwc{n} \hlstd{= n,} \hlkwc{size} \hlstd{=} \hlnum{2}\hlstd{,} \hlkwc{prob} \hlstd{= currThetaValue)}

    \hlcom{# calculate the value of theta0, theta1 and theta2}
    \hlstd{temp0[j]} \hlkwb{=} \hlnum{1} \hlopt{-} \hlkwd{sqrt}\hlstd{(}\hlkwd{sum}\hlstd{(obs} \hlopt{==} \hlnum{0}\hlstd{)}\hlopt{/}\hlstd{n)}
    \hlstd{temp1[j]} \hlkwb{=} \hlnum{1}\hlopt{/}\hlnum{2} \hlopt{*} \hlkwd{mean}\hlstd{(obs)}
    \hlstd{temp2[j]} \hlkwb{=} \hlkwd{sqrt}\hlstd{(}\hlkwd{sum}\hlstd{(obs} \hlopt{==} \hlnum{2}\hlstd{)}\hlopt{/}\hlstd{n)}

  \hlstd{\}}

  \hlstd{temp0} \hlkwb{=} \hlstd{(temp0} \hlopt{-} \hlstd{currThetaValue)}\hlopt{^}\hlnum{2}
  \hlstd{temp1} \hlkwb{=} \hlstd{(temp1} \hlopt{-} \hlstd{currThetaValue)}\hlopt{^}\hlnum{2}
  \hlstd{temp2} \hlkwb{=} \hlstd{(temp2} \hlopt{-} \hlstd{currThetaValue)}\hlopt{^}\hlnum{2}

  \hlstd{mse0[i]} \hlkwb{=} \hlkwd{mean}\hlstd{(temp0)}
  \hlstd{mse1[i]} \hlkwb{=} \hlkwd{mean}\hlstd{(temp1)}
  \hlstd{mse2[i]} \hlkwb{=} \hlkwd{mean}\hlstd{(temp2)}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}
After all this processing, we can now plot the values of the mean squared error for each value of $\theta_k$ (where $k \in \cbrac{1, 2, 3, ..., 39}$). We also plot the 
Cramer Rao Lower Bound and show the result below:

\begin{knitrout}\small
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcom{# returns the min and max value for the range}
\hlstd{yRange} \hlkwb{=} \hlkwd{range}\hlstd{(}\hlkwd{c}\hlstd{(mse0, mse1, mse2))}
\hlcom{# plot the best estimator theta1 hat}
\hlkwd{plot}\hlstd{(}\hlkwc{x} \hlstd{= thetaVals,} \hlkwc{y} \hlstd{= mse1,} \hlkwc{col} \hlstd{=} \hlstr{"red"}\hlstd{,} \hlkwc{ylim} \hlstd{= yRange,}
     \hlkwc{type} \hlstd{=} \hlstr{"l"}\hlstd{,} \hlkwc{main} \hlstd{=} \hlstr{"Empirical MSE's"}\hlstd{,}
     \hlkwc{xlab} \hlstd{=} \hlstr{"Theta values"}\hlstd{,}
     \hlkwc{ylab} \hlstd{=} \hlstr{"mean squared error"}\hlstd{)}
\hlcom{# plot theta0 hat}
\hlkwd{lines}\hlstd{(}\hlkwc{x} \hlstd{= thetaVals,} \hlkwc{y} \hlstd{= mse0,} \hlkwc{col} \hlstd{=} \hlstr{"blue"}\hlstd{)}
\hlcom{# plot theta2 hat}
\hlkwd{lines}\hlstd{(}\hlkwc{x} \hlstd{= thetaVals,} \hlkwc{y} \hlstd{= mse2,} \hlkwc{col} \hlstd{=} \hlstr{"DarkGreen"}\hlstd{)}
\hlcom{# plot the CRLB}
\hlkwd{curve}\hlstd{(}\hlnum{0.5} \hlopt{*} \hlstd{x} \hlopt{*} \hlstd{(}\hlnum{1} \hlopt{-} \hlstd{x)}\hlopt{/}\hlstd{n,} \hlkwc{add} \hlstd{=} \hlnum{TRUE}\hlstd{,} \hlkwc{lty} \hlstd{=} \hlnum{2}\hlstd{)}
\hlcom{# legend}
\hlkwd{legend}\hlstd{(}\hlkwc{x} \hlstd{=} \hlstr{"top"}\hlstd{,}
       \hlkwc{legend} \hlstd{=} \hlkwd{c}\hlstd{(}\hlstr{"using average"}\hlstd{,} \hlstr{"using N0"}\hlstd{,} \hlstr{"using N2"}\hlstd{,} \hlstr{"CRLB"}\hlstd{),}
       \hlkwc{col} \hlstd{=} \hlkwd{c}\hlstd{(}\hlstr{"red"}\hlstd{,} \hlstr{"blue"}\hlstd{,} \hlstr{"DarkGreen"}\hlstd{,} \hlstr{"black"}\hlstd{),}
       \hlkwc{lty} \hlstd{=} \hlkwd{c}\hlstd{(}\hlnum{1}\hlstd{,} \hlnum{1}\hlstd{,} \hlnum{1}\hlstd{,} \hlnum{2}\hlstd{))}
\end{alltt}
\end{kframe}

{\centering \includegraphics[width=\maxwidth]{figure/unnamed-chunk-3-1} 
>>>>>>> c054bc4aaf0712fca6c742a0011065c3496f8367

}

<<<<<<< HEAD
\begin{Schunk}
\begin{Sinput}
> 
\end{Sinput}
\end{Schunk}
=======

\end{knitrout}

}
\end{enumerate}
>>>>>>> c054bc4aaf0712fca6c742a0011065c3496f8367

\end{document}
