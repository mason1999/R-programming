---
title: "DATA2002: Lecture 32"
subtitle: "Clustering"
author: "Garth Tarr"
output:
  xaringan::moon_reader:
    css: ["default", "assets/sydney-fonts.css", "assets/sydney.css"]
    self_contained: false # if true, fonts will be stored locally
    seal: true # show a title slide with YAML information
    includes:
      in_header: "assets/mathjax-equation-numbers.html"
    nature:
      beforeInit: ["assets/remark-zoom.js", "https://platform.twitter.com/widgets.js"]
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: '16:9' # alternatives '16:9' or '4:3' or others e.g. 13:9
      navigation:
        scroll: false # disable slide transitions by scrolling
---
class: segue

```{r Lec32, echo=FALSE, warning=FALSE, message=FALSE, cache=FALSE}
source("../assets/lec_first_chunk.R")
```

```{r generate_all, eval = FALSE, echo=FALSE, message = FALSE}
lec = "32"
# rmarkdown::render(paste("lec",lec,".Rmd",sep=""), quiet = TRUE)
knitr::purl(paste("lec",lec,".Rmd",sep=""),
            output = paste("lec",lec,".R",sep=""))
pagedown::chrome_print(paste("lec",lec,".html",sep=""))
```

.large[
Unsupervised learning

Hierarchical clustering

k-means clustering
]


---
class: segue

# Unsupervised learning

---

## Can we find things that are close together? 

> When an observation consists of a set of numerical variables but none of them are considered a "response" variable, we might like to **cluster** them in some way.

* How do we define close?
* How do we group things?
* How do we visualise the grouping? 
* How do we interpret the grouping? 


---

.center[
<img src=imgs/cluster_scholar_oct_2018.png width=80%>
]


.footnote[
[Google Scholar: cluster analysis](http://scholar.google.com/scholar?hl=en&q=cluster+analysis)
]

---
class: segue

## Hierarchical clustering

---

## Hierarchical clustering

.pull-left-2[
* An agglomerative approach (bottom-up)
  * Find closest two things
  * Put them together
  * Find next closest
* Requires
  * A defined distance
  * A merging approach
* Produces
  * A "tree" showing how close things are to each other
]
.pull-right-1[
<img src=imgs/iris_dendrogram.png>
]


???

```{r, eval = FALSE}
iris <- datasets::iris
iris2 <- iris[,-5]
species_labels <- iris[,5]
library(colorspace) # get nice colors
species_col <- rev(rainbow_hcl(3))[as.numeric(species_labels)]

d_iris <- dist(iris2) # method="man" # is a bit better
hc_iris <- hclust(d_iris, method = "complete")
iris_species <- rev(levels(iris[,5]))

library(dendextend)
dend <- as.dendrogram(hc_iris)
# order it the closest we can to the order of the observations:
dend <- rotate(dend, 1:150)

# Color the branches based on the clusters:
dend <- color_branches(dend, k=3) #, groupLabels=iris_species)

# Manually match the labels, as much as possible, to the real classification of the flowers:
labels_colors(dend) <-
   rainbow_hcl(3)[sort_levels_values(
      as.numeric(iris[,5])[order.dendrogram(dend)]
   )]

# We shall add the flower type to the labels:
labels(dend) <- paste(as.character(iris[,5])[order.dendrogram(dend)],
                           "(",labels(dend),")", 
                           sep = "")
# We hang the dendrogram a bit:
dend <- hang.dendrogram(dend,hang_height=0.1)
# reduce the size of the labels:
# dend <- assign_values_to_leaves_nodePar(dend, 0.5, "lab.cex")
dend <- set(dend, "labels_cex", 0.5)
# And plot:
par(mar = c(3,3,3,7))
plot(dend, 
     main = "Clustered Iris data set
     (the labels give the true flower species)", 
     horiz =  TRUE,  nodePar = list(cex = .007))
legend("topleft", legend = iris_species, fill = rainbow_hcl(3))
```

---

## How do we define close?

- Most important step
    - Garbage in garbage out (GIGO)
- Distance or similarity
    - Continuous
        - Euclidean distance
        - correlation similarity
    - Binary 
        - Manhattan distance
- Pick a distance/similarity that makes sense for your problem
  

---

## Example distances - Manhattan vs Euclidean

.pull-left-1[

<img class=center src=imgs/manhattan.svg width = 100%>

]
.pull-right-2[

Manhattan geometry versus Euclidean distance: 

-   In Manhattan (taxicab) geometry, the red, yellow and blue paths all have the same shortest path length of 12. 

-   In Euclidean geometry, the green line has length ${\displaystyle 6{\sqrt {2}}\approx 8.49} 6{\sqrt {2}}\approx 8.49$, and is the unique shortest path.


In general for Manhattan:

$$|A_1-A_2| + |B_1-B_2| + \ldots + |Z_1-Z_2|$$


In general for Euclidean:

$$\sqrt{(A_1-A_2)^2 + (B_1-B_2)^2 + \ldots + (Z_1-Z_2)^2}$$


]

.footnote[
[http://en.wikipedia.org/wiki/Taxicab_geometry](http://en.wikipedia.org/wiki/Taxicab_geometry) and [Canberra distance](https://blog.csiro.au/going-nuts-with-the-canberra-distance/)

]


---

## Hierarchical clustering - example

.pull-left-2[
```{r createData_h, message = FALSE}
set.seed(1234)
library(tidyverse)
df = data.frame(
  x = rnorm(12, mean = rep(1:3, each = 4), sd = 0.2),
  y = rnorm(12, mean = rep(c(1,2,1), each = 4), sd = 0.2))
dist(df) %>% round(2)  #<<
```
The `dist()` function accepts parameters `x` (the data) and `method` e.g. `"euclidean"` (the default) or `"manhattan"`...
]
.pull-right-1[
```{r, message=FALSE, fig.height = 9}
library(ggrepel)
p = ggplot(df) +
  aes(x = x, y = y, label = 1:12) + 
  geom_point(size = 10) + 
  geom_text_repel(colour = "blue", 
    box.padding = 0.5,
    segment.size = 0, size = 14) +
  theme_classic(base_size = 46)
p
```
]

---

## Distances

The output of `dist(df)` gives the distance from each point to every other point.

.pull-left[
```{r heatmap1, fig.show="hide"}
distances = dist(df)
dist_mat = as.matrix(distances)
dist_df = data.frame(dist_mat,
                     check.names = FALSE) %>% 
  rownames_to_column(var="id1")
dist_long = gather(dist_df, 
                   key = id2, 
                   value = distance, 
                   -id1)
dist_long = dist_long %>% 
  mutate_at(
    .vars = vars(id1, id2),
    .funs = funs(factor(., levels = 1:12)))
ggplot(dist_long,
       aes(x = id1, y = id2, fill = distance)) + 
  geom_tile() + 
  scale_fill_viridis_c() + 
  theme_classic(base_size = 40)
diag(dist_mat) = diag(dist_mat) + 1e5
```
]
.pull-right[
```{r heatmap1, echo = FALSE, fig.height = 10}
```
]

---

## Hierarchical clustering - #1

.pull-left[
```{r hclust1, fig.show="hide", dependson="createData_h"}
ind = which(dist_mat == min(dist_mat),
            arr.ind=TRUE)
df$clust = "0"
df$clust[ind[,1]] = "1"
p1 = p + geom_point(aes(colour = clust), 
               data = df, size = 10) + 
  scale_color_brewer(palette = "Set1") + 
  theme(legend.position = "none")
p1
```

```{r hclust1, fig.height=7, echo = FALSE}
```

]
.pull-right[

```{r hclust2, fig.show = "hide"}
hcluster = hclust(distances)
dendro = as.dendrogram(hcluster)
par(mar = c(1,4,0.5,0.5), cex = 3)
plot(dendro)
```

```{r hclust2, fig.height=8, echo = FALSE}
```
]

---

## Hierarchical clustering - #2

.pull-left[
```{r, warning = FALSE, fig.height=8}
mean_df = df %>% filter(clust == "1") %>% 
  summarise_if(.predicate = is.numeric,
               .funs = mean)
p2 = p1 + geom_point(aes(x = x, y = y, label = ""),
                data = mean_df, pch = 18, 
                size = 10, col = "orange")
p2
```
]
.pull-right[
Now we find the next "closest" observation, treating the "mean of 5 and 6" as a point for consideration.
]

---

## Hierarchical clustering - #3

.pull-left[
```{r dependson="createData_h", fig.height = 8}
ind = which(dist_mat == dist_mat[order(dist_mat)][3],
            arr.ind=TRUE)
df$clust[ind[,1]] = "2"
p3 = p2 + geom_point(aes(colour = clust), 
               data = df, size = 10)
p3
```
]
.pull-right[
Observations 10 and 11 are the next closest,
```{r hclust2, fig.height=8, echo = FALSE}
```
and so on...
]

---

## Visualising hierarchical clusters

The result of this hierarchical clustering can be visualised as a  **dendrogram** or a **phylogenetic tree**.

```{r, fig.height=6}
library(ape)
distances %>% hclust() %>% as.phylo() %>% plot(cex = 2)
```

---

## Visualising correlation matrices

```{r, message = FALSE}
library(heatmaply)
heatmaply(cor(mtcars), margins = c(50, 50), k_col = 2, k_row = 2, limits = c(-1,1))
```

---

## Notes and further resources

* Gives an idea of the relationships between variables/observations
* The picture may be unstable
  * Change a few points
  * Have different missing values
  * Pick a different distance
  * Change the merging strategy
  * Change the scale of points for one variable
* But it is deterministic
* Choosing where to cut isn't always obvious
* Should be primarily used for exploration 
* [Rafa's Distances and Clustering Video](http://www.youtube.com/watch?v=wQhVWUcXM0A)
* [Elements of statistical learning](http://www-stat.stanford.edu/~tibs/ElemStatLearn/)



---
class: segue

# k-means clustering

---

## Clustering cities into 6 regions (continents?)

.pull-left-1[
```{r, message=FALSE}
library(mdsr)
BigCities = WorldCities %>%
  arrange(desc(population)) %>%
  head(4000) %>%
  select(longitude, latitude)
glimpse(BigCities)
set.seed(15)
library(mclust)
city_clusts <- BigCities %>%
  kmeans(centers = 6) %>%
  fitted("classes") %>%
  as.character()
BigCities = BigCities %>% 
  mutate(cluster = city_clusts)
```
]
.pull-right-2[
```{r}
BigCities %>% ggplot(aes(x = longitude, y = latitude)) +
  geom_point(aes(color = cluster), alpha = 0.5) + 
  theme_classic(base_size = 36)
```
]


---

## Clustering cities into 5 regions

.pull-left-1[
```{r, message=FALSE}
city_clusts <- BigCities %>%
  kmeans(centers = 5) %>%
  fitted("classes") %>%
  as.character()
BigCities = BigCities %>% 
  mutate(cluster = city_clusts)
```
]
.pull-right-2[
```{r}
BigCities %>% ggplot(aes(x = longitude, y = latitude)) +
  geom_point(aes(color = cluster), alpha = 0.5) + 
  theme_classic(base_size = 36)
```
]



---

## Clustering cities into 5 regions (again)

.pull-left-1[
```{r, message=FALSE}
city_clusts <- BigCities %>%
  kmeans(centers = 5) %>%
  fitted("classes") %>%
  as.character()
BigCities = BigCities %>% 
  mutate(cluster = city_clusts)
```

> Hmmmm.... this isn't the same as we saw on the previous slide?!?

]
.pull-right-2[
```{r}
BigCities %>% ggplot(aes(x = longitude, y = latitude)) +
  geom_point(aes(color = cluster), alpha = 0.5) + 
  theme_classic(base_size = 36)
```
]



---

## K-means clustering

.pull-left[
* A partitioning approach (there's no hierarchy)
  * Fix a number of clusters
  * Get "centroids" of each cluster
  * Assign things to closest centroid
  * Recalculate centroids
* Requires
  * A defined distance metric
  * A number of clusters
  * An initial guess as to cluster centroids
* Produces
  * Final estimate of cluster centroids
  * An assignment of each point to clusters
]
.pull-right[
This is somewhat like kNN except there's no response variable.  The definition of the groups must be inferred from the data
]

---

## K-means basic idea

-   The basic idea behind k-means clustering consists of defining clusters so that the total intra-cluster variation (known as total within-cluster variation) is minimised.

-   There are several k-means algorithms available. The standard algorithm is the Hartigan-Wong algorithm (Hartigan and Wong 1979), which defines the total within-cluster variation as the sum of squared distances Euclidean distances between items and the corresponding centroid:
$$W(C_k) = \sum_{x_i\in C_k}(x_i - \bar{x}_k)^2$$
where $x_i$ is an observation belonging to cluster $C_k$ and $\bar{x}_k$ is the mean of the points assigned to cluster $C_k$.

-   Each observation is assigned to a cluster such that the sum of squares distance of the observation to their assigned cluster centers is minimised.

-   The total within-cluster variation is:
$$\sum_{k} W(C_k) = \sum_{k}\sum_{x_i\in C_k}(x_i - \bar{x}_k)^2$$

-   The total within cluster sum of squares measures the compactness of the clustering - we want this to be as small as possible so that we have the best fit possible.

---

## K-means algorithm

K-means algorithm can be summarised as follow:

1.    Specify the number of clusters (k) to be created (by the analyst)
2.    Select randomly k objects from the dataset as the initial cluster centers or means
3.    Assign each observation to their closest centroid, based on the Euclidean distance between the object and the centroid
4.    For each of the k clusters update the cluster centroid by calculating the new mean values of all the data points in the cluster. The centroid of a kth cluster is a vector of length p containing the means of all variables for the observations in the kth cluster; p is the number of variables.
5.    Iteratively minimise the total within sum of square. That is, iterate steps 3 and 4 until the cluster assignments stop changing or the maximum number of iterations is reached. R uses 10 as the default value for the maximum number of iterations.


???

https://www.datanovia.com/en/lessons/k-means-clustering-in-r-algorith-and-practical-examples/

---

## K-means clustering -  example


```{r createData_k, fig.height=3.5,fig.width=3.5}
set.seed(1234); par(mar=c(0,0,0,0))
x = rnorm(12,mean=rep(1:3,each=4),sd=0.2)
y = rnorm(12,mean=rep(c(1,2,1),each=4),sd=0.2)
plot(x,y,col="blue",pch=19,cex=2)
text(x+0.05,y+0.05,labels=as.character(1:12))
```


---

## K-means clustering -  starting centroids


```{r,dependson="createData_k",echo=FALSE,fig.height=5,fig.width=5}
par(mar=rep(0.2,4))
plot(x,y,col="blue",pch=19,cex=2)
text(x+0.05,y+0.05,labels=as.character(1:12))
cx = c(1,1.8,2.5)
cy = c(2,1,1.5)
points(cx,cy,col=c("red","orange","purple"),pch=3,cex=2,lwd=2)
```

---

## K-means clustering -  assign to closest centroid

```{r,dependson="createData_k",echo=FALSE,fig.height=5,fig.width=5}
par(mar=rep(0.2,4))
plot(x,y,col="blue",pch=19,cex=2)
cols1 = c("red","orange","purple")
text(x+0.05,y+0.05,labels=as.character(1:12))
cx = c(1,1.8,2.5)
cy = c(2,1,1.5)
points(cx,cy,col=cols1,pch=3,cex=2,lwd=2)

## Find the closest centroid
distTmp = matrix(NA,nrow=3,ncol=12)
distTmp[1,] = (x-cx[1])^2 + (y-cy[1])^2
distTmp[2,] = (x-cx[2])^2 + (y-cy[2])^2
distTmp[3,] = (x-cx[3])^2 + (y-cy[3])^2
newClust = apply(distTmp,2,which.min)
points(x,y,pch=19,cex=2,col=cols1[newClust])
```

---

## K-means clustering -  recalculate centroids

```{r,dependson="createData_k",echo=FALSE,fig.height=5,fig.width=5}
par(mar=rep(0.2,4))
plot(x,y,col="blue",pch=19,cex=2)
cols1 = c("red","orange","purple")
text(x+0.05,y+0.05,labels=as.character(1:12))

## Find the closest centroid
distTmp = matrix(NA,nrow=3,ncol=12)
distTmp[1,] = (x-cx[1])^2 + (y-cy[1])^2
distTmp[2,] = (x-cx[2])^2 + (y-cy[2])^2
distTmp[3,] = (x-cx[3])^2 + (y-cy[3])^2
newClust = apply(distTmp,2,which.min)
points(x,y,pch=19,cex=2,col=cols1[newClust])
newCx = tapply(x,newClust,mean)
newCy = tapply(y,newClust,mean)

## Old centroids

cx = c(1,1.8,2.5)
cy = c(2,1,1.5)

points(newCx,newCy,col=cols1,pch=3,cex=2,lwd=2)

```


---

## K-means clustering -  reassign values

```{r,dependson="createData_k",echo=FALSE,fig.height=5,fig.width=5}
par(mar=rep(0.2,4))
plot(x,y,col="blue",pch=19,cex=2)
cols1 = c("red","orange","purple")
text(x+0.05,y+0.05,labels=as.character(1:12))


cx = c(1,1.8,2.5)
cy = c(2,1,1.5)


## Find the closest centroid
distTmp = matrix(NA,nrow=3,ncol=12)
distTmp[1,] = (x-cx[1])^2 + (y-cy[1])^2
distTmp[2,] = (x-cx[2])^2 + (y-cy[2])^2
distTmp[3,] = (x-cx[3])^2 + (y-cy[3])^2
newClust = apply(distTmp,2,which.min)
newCx = tapply(x,newClust,mean)
newCy = tapply(y,newClust,mean)

## Old centroids

points(newCx,newCy,col=cols1,pch=3,cex=2,lwd=2)


## Iteration 2
distTmp = matrix(NA,nrow=3,ncol=12)
distTmp[1,] = (x-newCx[1])^2 + (y-newCy[1])^2
distTmp[2,] = (x-newCx[2])^2 + (y-newCy[2])^2
distTmp[3,] = (x-newCx[3])^2 + (y-newCy[3])^2
newClust2 = apply(distTmp,2,which.min)

points(x,y,pch=19,cex=2,col=cols1[newClust2])

```



---

## K-means clustering -  update centroids

```{r,dependson="createData_k",echo=FALSE,fig.height=5,fig.width=5}
par(mar=rep(0.2,4))
plot(x,y,col="blue",pch=19,cex=2)
cols1 = c("red","orange","purple")
text(x+0.05,y+0.05,labels=as.character(1:12))


cx = c(1,1.8,2.5)
cy = c(2,1,1.5)

## Find the closest centroid
distTmp = matrix(NA,nrow=3,ncol=12)
distTmp[1,] = (x-cx[1])^2 + (y-cy[1])^2
distTmp[2,] = (x-cx[2])^2 + (y-cy[2])^2
distTmp[3,] = (x-cx[3])^2 + (y-cy[3])^2
newClust = apply(distTmp,2,which.min)
newCx = tapply(x,newClust,mean)
newCy = tapply(y,newClust,mean)



## Iteration 2
distTmp = matrix(NA,nrow=3,ncol=12)
distTmp[1,] = (x-newCx[1])^2 + (y-newCy[1])^2
distTmp[2,] = (x-newCx[2])^2 + (y-newCy[2])^2
distTmp[3,] = (x-newCx[3])^2 + (y-newCy[3])^2
finalClust = apply(distTmp,2,which.min)


## Final centroids
finalCx = tapply(x,finalClust,mean)
finalCy = tapply(y,finalClust,mean)
points(finalCx,finalCy,col=cols1,pch=3,cex=2,lwd=2)



points(x,y,pch=19,cex=2,col=cols1[finalClust])

```


---

## The `kmeans()`function

.pull-left[
* Important parameters: `x`, `centers`, `iter.max`, `nstart`

```{r kmeans,dependson="createData_k"}
df = data.frame(x,y)
kmeansObj = kmeans(df, centers = 3)
names(kmeansObj)
kmeansObj$cluster
```
]
.pull-right[
```{r}
df$kmeans_class = kmeansObj$cluster
df
```
]

<!--
## `autoplot()`

```{r, dependson="kmeans"}
library(ggfortify)
autoplot(kmeansObj, data = df)
```

-->

---

## US arrests data

.pull-left-1[
```{r}
data("USArrests")
glimpse(USArrests)
df = scale(USArrests)
```
]
.pull-right-2[
```{r, fig.height=8, message = FALSE}
GGally::ggpairs(df) + theme_bw(base_size = 30)
```
]

---

## How to choose $k$?

.pull-left[
1.  Look at your data
2.  Consider the "within sum of squares" for different values of $k$
    -   The location of a kink in a plot of within sum of squares against $k$ can provide an indication of a "good" value for $k$.
]

.pull-right[
```{r, message = FALSE, fig.height=8.5}
library(factoextra)
fviz_nbclust(df, FUNcluster = kmeans, method = "wss") + 
  geom_vline(xintercept = 4, linetype = 2) + 
  theme_classic(base_size = 36)
```
]


---

## Compute the clusters

.scroll-output[
```{r}
set.seed(1)
km = kmeans(df, centers = 4, nstart = 10)
km
```
]

???

As the final result of k-means clustering result is sensitive to the random starting assignments, we specify nstart = 25. This means that R will try 25 different random starting assignments and then select the best results corresponding to the one with the lowest within cluster variation. The default value of nstart in R is one. But, it's strongly recommended to compute k-means clustering with a large value of nstart such as 25 or 50, in order to have a more stable result.

---

## Visualise the clusters

.pull-left[
How to visualise the clusters when we have more than 2 variables???

```{r}
df2 = df %>% 
  data.frame() %>%
  rownames_to_column(var = "state") %>%
  mutate(cluster = factor(km$cluster))
glimpse(df2)
```

]
.pull-right[
```{r, fig.height=8, message = FALSE}
df2 %>% select(-state) %>% 
  GGally::ggpairs(aes(colour = cluster)) + theme_bw(base_size = 30)
```
]

---

## Visualise the clusters using PCA

We can perform **dimension reduction** to "project" our observations onto a 2-dimensional subspace from the original 4-dimensional space. (See the next lecture for details.)

```{r, fig.height = 5.5}
fviz_cluster(km, data = df) + theme_classic(base_size = 26)
```

---

## Notes and further resources

* K-means requires a number of clusters
  * Pick by eye/intuition
  * Consider trade off between $k$ and minimising within sum of squares
  * Pick by cross validation/information theory, etc.
  * [Determining the number of clusters](http://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set)
* K-means is not deterministic
  * Different # of clusters 
  * Different number of iterations
* [Rafael Irizarry's Distances and Clustering Video](http://www.youtube.com/watch?v=wQhVWUcXM0A)
* [Elements of statistical learning](http://www-stat.stanford.edu/~tibs/ElemStatLearn/)

---

## Hierarchical vs k-means

In a hierarchical approach you 
-   don't need to specify the number of clusters
-   can cut the hierarchy at any level
-   can easily interpret for various applications
-   have a deterministic outcome

In comparison, k-means clustering
-   can be much faster than hierarchical (computation time is linear in $n$ vs hierarchical which is quadratic in $n$)
-   can incorporate new data and reform clusters easily
-   generates flat partitions
-   more adaptive (an observation can swap between clusters when the centroids are re-computed)


---

## References

Hartigan, JA, and MA Wong. 1979. “Algorithm AS 136: A K-means clustering algorithm.” Applied Statistics. Royal Statistical Society, 100–108.

MacQueen, J. 1967. “Some Methods for Classification and Analysis of Multivariate Observations.” In Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Statistics, 281–97. Berkeley, Calif.: University of California Press. http://projecteuclid.org:443/euclid.bsmsp/1200512992.
